"""
ORFEAS AI 2DÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢3D Studio - Performance Profiler
===============================================
ORFEAS AI Project

Comprehensive performance profiling and bottleneck detection

Features:
- Pipeline stage timing analysis
- Bottleneck identification
- Performance reports and recommendations
- Historical trend analysis
"""

import logging
import time
import cProfile
import pstats
import io
from typing import Dict, Any, List, Optional, Callable, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from contextlib import contextmanager
import json
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class StageProfile:
    """Performance profile for a single pipeline stage"""
    name: str
    duration_seconds: float
    timestamp: float
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def duration_ms(self) -> float:
        """Duration in milliseconds"""
        return self.duration_seconds * 1000


@dataclass
class PipelineProfile:
    """Complete pipeline execution profile"""
    pipeline_id: str
    stages: List[StageProfile]
    total_duration: float
    timestamp: float
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def slowest_stage(self) -> Optional[StageProfile]:
        """Get the slowest stage"""
        if not self.stages:
            return None
        return max(self.stages, key=lambda s: s.duration_seconds)

    @property
    def stage_percentages(self) -> Dict[str, float]:
        """Get percentage of time spent in each stage"""
        if self.total_duration == 0:
            return {}
        return {
            stage.name: (stage.duration_seconds / self.total_duration) * 100
            for stage in self.stages
        }


class PerformanceProfiler:
    """
    Performance profiling and analysis

    Tracks pipeline execution, identifies bottlenecks, generates reports.
    """

    def __init__(self):
        """Initialize performance profiler"""
        self.pipeline_profiles: List[PipelineProfile] = []
        self.current_pipeline: Optional[Dict[str, Any]] = None
        self.stage_start_times: Dict[str, float] = {}

        # Configuration
        self.max_stored_profiles = 100
        self.enable_cprofile = False  # Disabled by default (high overhead)

        logger.info("[ORFEAS] Performance Profiler initialized")

    def start_pipeline(self, pipeline_id: str, metadata: Optional[Dict[str, Any]] = None):
        """
        Start profiling a new pipeline execution

        Args:
            pipeline_id: Unique identifier for this pipeline run
            metadata: Additional context (job_id, image_size, etc.)
        """
        self.current_pipeline = {
            'id': pipeline_id,
            'stages': [],
            'start_time': time.time(),
            'metadata': metadata or {}
        }
        logger.debug(f"[ORFEAS] Started profiling pipeline: {pipeline_id}")

    def end_pipeline(self) -> Optional[PipelineProfile]:
        """
        End current pipeline profiling

        Returns:
            PipelineProfile with complete execution data
        """
        if not self.current_pipeline:
            logger.warning("[ORFEAS] No active pipeline to end")
            return None

        end_time = time.time()
        total_duration = end_time - self.current_pipeline['start_time']

        profile = PipelineProfile(
            pipeline_id=self.current_pipeline['id'],
            stages=self.current_pipeline['stages'],
            total_duration=total_duration,
            timestamp=end_time,
            metadata=self.current_pipeline['metadata']
        )

        # Store profile
        self.pipeline_profiles.append(profile)
        if len(self.pipeline_profiles) > self.max_stored_profiles:
            self.pipeline_profiles.pop(0)

        logger.info(f"[ORFEAS] Pipeline {profile.pipeline_id}: {total_duration:.2f}s total")

        # Log slowest stage
        if profile.slowest_stage:
            pct = profile.stage_percentages[profile.slowest_stage.name]
            logger.info(f"[ORFEAS] Slowest stage: {profile.slowest_stage.name} ({pct:.1f}%)")

        # Clear current pipeline
        self.current_pipeline = None

        return profile

    @contextmanager
    def profile_stage(self, stage_name: str, metadata: Optional[Dict[str, Any]] = None):
        """
        Context manager for profiling a pipeline stage

        Args:
            stage_name: Name of the stage
            metadata: Additional context

        Usage:
            with profiler.profile_stage('image_preprocessing'):
                preprocess_image(image)
        """
        if not self.current_pipeline:
            logger.warning(f"[ORFEAS] No active pipeline for stage: {stage_name}")
            yield
            return

        start_time = time.time()
        logger.debug(f"[ORFEAS] Starting stage: {stage_name}")

        try:
            yield
        finally:
            end_time = time.time()
            duration = end_time - start_time

            stage_profile = StageProfile(
                name=stage_name,
                duration_seconds=duration,
                timestamp=end_time,
                metadata=metadata or {}
            )

            self.current_pipeline['stages'].append(stage_profile)
            logger.debug(f"[ORFEAS] Completed stage {stage_name}: {duration:.3f}s")

    def get_average_stage_duration(self, stage_name: str, window_size: int = 20) -> float:
        """
        Get average duration for a specific stage

        Args:
            stage_name: Name of the stage
            window_size: Number of recent profiles to average

        Returns:
            Average duration in seconds
        """
        recent_profiles = self.pipeline_profiles[-window_size:]
        durations = []

        for profile in recent_profiles:
            for stage in profile.stages:
                if stage.name == stage_name:
                    durations.append(stage.duration_seconds)

        if not durations:
            return 0.0

        return sum(durations) / len(durations)

    def identify_bottlenecks(self, threshold_percent: float = 20.0) -> List[Dict[str, Any]]:
        """
        Identify pipeline bottlenecks

        Args:
            threshold_percent: Stage is a bottleneck if it takes > this % of total time

        Returns:
            List of bottleneck information dicts
        """
        if not self.pipeline_profiles:
            return []

        # Analyze recent profiles
        recent_profiles = self.pipeline_profiles[-20:]

        # Aggregate stage statistics
        stage_stats: Dict[str, List[float]] = {}
        for profile in recent_profiles:
            percentages = profile.stage_percentages
            for stage_name, percent in percentages.items():
                if stage_name not in stage_stats:
                    stage_stats[stage_name] = []
                stage_stats[stage_name].append(percent)

        # Find bottlenecks (stages consistently > threshold)
        bottlenecks = []
        for stage_name, percentages in stage_stats.items():
            avg_percent = sum(percentages) / len(percentages)
            if avg_percent > threshold_percent:
                avg_duration = self.get_average_stage_duration(stage_name)
                bottlenecks.append({
                    'stage': stage_name,
                    'avg_percent': avg_percent,
                    'avg_duration_seconds': avg_duration,
                    'occurrences': len(percentages),
                    'severity': 'critical' if avg_percent > 40 else 'high' if avg_percent > 30 else 'medium'
                })

        # Sort by severity
        bottlenecks.sort(key=lambda x: x['avg_percent'], reverse=True)

        return bottlenecks

    def get_performance_summary(self) -> Dict[str, Any]:
        """
        Get comprehensive performance summary

        Returns:
            Dict with performance statistics
        """
        if not self.pipeline_profiles:
            return {
                'total_pipelines': 0,
                'avg_duration': 0,
                'error': 'No pipeline data available'
            }

        recent_profiles = self.pipeline_profiles[-50:]

        # Calculate statistics
        total_durations = [p.total_duration for p in recent_profiles]
        avg_duration = sum(total_durations) / len(total_durations)
        min_duration = min(total_durations)
        max_duration = max(total_durations)

        # Stage breakdown
        stage_durations: Dict[str, List[float]] = {}
        for profile in recent_profiles:
            for stage in profile.stages:
                if stage.name not in stage_durations:
                    stage_durations[stage.name] = []
                stage_durations[stage.name].append(stage.duration_seconds)

        stage_breakdown = {
            name: {
                'avg_duration': sum(durations) / len(durations),
                'min_duration': min(durations),
                'max_duration': max(durations),
                'count': len(durations)
            }
            for name, durations in stage_durations.items()
        }

        # Identify bottlenecks
        bottlenecks = self.identify_bottlenecks(threshold_percent=20.0)

        return {
            'total_pipelines': len(recent_profiles),
            'avg_duration': avg_duration,
            'min_duration': min_duration,
            'max_duration': max_duration,
            'stage_breakdown': stage_breakdown,
            'bottlenecks': bottlenecks,
            'timestamp': datetime.utcnow().isoformat()
        }

    def get_optimization_recommendations(self) -> List[str]:
        """
        Get actionable optimization recommendations based on profiling data

        Returns:
            List of recommendation strings
        """
        recommendations = []

        if not self.pipeline_profiles:
            recommendations.append("ÃƒÂ¢Ã…Â¡Ã‚Â ÃƒÂ¯Ã‚Â¸Ã‚Â No profiling data available - run some pipelines first")
            return recommendations

        summary = self.get_performance_summary()
        bottlenecks = summary.get('bottlenecks', [])

        # Bottleneck recommendations
        if bottlenecks:
            for bottleneck in bottlenecks[:3]:  # Top 3
                stage = bottleneck['stage']
                percent = bottleneck['avg_percent']
                duration = bottleneck['avg_duration_seconds']

                recommendations.append(
                    f"ÃƒÂ°Ã…Â¸Ã¢â‚¬ÂÃ‚Â´ {bottleneck['severity'].upper()}: '{stage}' takes {percent:.1f}% "
                    f"({duration:.2f}s) - optimize this stage first"
                )

                # Stage-specific recommendations
                if 'preprocessing' in stage.lower():
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Consider caching preprocessed images")
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Optimize image resizing with PIL-SIMD or pillow-avif-plugin")
                elif 'shape_generation' in stage.lower() or 'inference' in stage.lower():
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Enable mixed precision (FP16) for faster inference")
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Reduce inference steps (50 ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ 30-40) if quality permits")
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Consider model quantization (INT8)")
                elif 'texture' in stage.lower():
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Use lower resolution textures for non-critical cases")
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Enable texture compression")
                elif 'postprocessing' in stage.lower() or 'export' in stage.lower():
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Use binary STL format (faster than ASCII)")
                    recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Optimize mesh decimation algorithms")
        else:
            recommendations.append("ÃƒÂ¢Ã…â€œÃ¢â‚¬Â¦ No major bottlenecks detected - pipeline well-balanced")

        # Performance trend recommendations
        avg_duration = summary['avg_duration']
        if avg_duration > 100:
            recommendations.append(f"ÃƒÂ¢Ã‚ÂÃ‚Â±ÃƒÂ¯Ã‚Â¸Ã‚Â Average duration {avg_duration:.1f}s exceeds target (60s)")
            recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Target 45% improvement to reach <60s goal")
        elif avg_duration < 60:
            recommendations.append(f"ÃƒÂ¢Ã…â€œÃ¢â‚¬Â¦ Average duration {avg_duration:.1f}s meets performance target!")

        # Variance recommendations
        if summary['max_duration'] > summary['min_duration'] * 2:
            recommendations.append("ÃƒÂ°Ã…Â¸Ã¢â‚¬Å“Ã…Â  High variance detected - performance inconsistent")
            recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Investigate GPU memory contention")
            recommendations.append("   ÃƒÂ¢Ã¢â‚¬Â Ã¢â‚¬â„¢ Check for I/O bottlenecks")

        return recommendations

    def export_report(self, output_path: str = "performance_report.json"):
        """
        Export performance report to JSON

        Args:
            output_path: Path to save report
        """
        summary = self.get_performance_summary()
        recommendations = self.get_optimization_recommendations()

        report = {
            'generated_at': datetime.utcnow().isoformat(),
            'summary': summary,
            'recommendations': recommendations,
            'recent_pipelines': [
                {
                    'id': p.pipeline_id,
                    'duration': p.total_duration,
                    'timestamp': p.timestamp,
                    'stages': [
                        {
                            'name': s.name,
                            'duration': s.duration_seconds,
                            'percent': p.stage_percentages.get(s.name, 0)
                        }
                        for s in p.stages
                    ]
                }
                for p in self.pipeline_profiles[-10:]
            ]
        }

        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)

        logger.info(f"[ORFEAS] Performance report exported to {output_path}")

        return report

    def profile_function(self, func: Callable, *args, **kwargs) -> Tuple[Any, float]:
        """
        Profile a single function execution

        Args:
            func: Function to profile
            *args: Function arguments
            **kwargs: Function keyword arguments

        Returns:
            Tuple of (function_result, duration_seconds)
        """
        start_time = time.time()

        if self.enable_cprofile:
            profiler = cProfile.Profile()
            profiler.enable()

        try:
            result = func(*args, **kwargs)
        finally:
            end_time = time.time()
            duration = end_time - start_time

            if self.enable_cprofile:
                profiler.disable()
                # Print stats to logger
                s = io.StringIO()
                ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                ps.print_stats(10)  # Top 10 functions
                logger.debug(f"[ORFEAS] cProfile stats:\n{s.getvalue()}")

        return result, duration


# Singleton instance
_performance_profiler = None


def get_performance_profiler() -> PerformanceProfiler:
    """
    Get singleton performance profiler instance

    Returns:
        PerformanceProfiler instance
    """
    global _performance_profiler
    if _performance_profiler is None:
        _performance_profiler = PerformanceProfiler()
    return _performance_profiler


if __name__ == "__main__":
    # Standalone test
    print("=" * 80)
    print("ORFEAS Performance Profiler - Standalone Test")
    print("=" * 80)

    profiler = get_performance_profiler()

    # Simulate a pipeline execution
    print("\n1. Simulating Pipeline Execution...")
    profiler.start_pipeline('test_pipeline_001', {'image_size': (512, 512)})

    # Simulate stages
    import time

    with profiler.profile_stage('image_preprocessing'):
        time.sleep(0.1)  # 100ms

    with profiler.profile_stage('shape_generation'):
        time.sleep(0.5)  # 500ms (bottleneck)

    with profiler.profile_stage('texture_synthesis'):
        time.sleep(0.3)  # 300ms

    with profiler.profile_stage('mesh_export'):
        time.sleep(0.05)  # 50ms

    profile = profiler.end_pipeline()

    print(f"   Total Duration: {profile.total_duration:.3f}s")
    print(f"   Slowest Stage: {profile.slowest_stage.name}")

    # Add more profiles for statistics
    print("\n2. Simulating Additional Pipelines...")
    for i in range(5):
        profiler.start_pipeline(f'test_pipeline_{i:03d}', {'image_size': (512, 512)})

        with profiler.profile_stage('image_preprocessing'):
            time.sleep(0.08 + i * 0.01)

        with profiler.profile_stage('shape_generation'):
            time.sleep(0.45 + i * 0.02)

        with profiler.profile_stage('texture_synthesis'):
            time.sleep(0.25 + i * 0.015)

        with profiler.profile_stage('mesh_export'):
            time.sleep(0.04 + i * 0.005)

        profiler.end_pipeline()

    print(f"   Generated {len(profiler.pipeline_profiles)} pipeline profiles")

    # Get performance summary
    print("\n3. Performance Summary:")
    summary = profiler.get_performance_summary()
    print(f"   Average Duration: {summary['avg_duration']:.3f}s")
    print(f"   Min Duration: {summary['min_duration']:.3f}s")
    print(f"   Max Duration: {summary['max_duration']:.3f}s")

    # Identify bottlenecks
    print("\n4. Identified Bottlenecks:")
    bottlenecks = profiler.identify_bottlenecks(threshold_percent=20.0)
    for i, bottleneck in enumerate(bottlenecks, 1):
        print(f"   {i}. {bottleneck['stage']}: {bottleneck['avg_percent']:.1f}% "
              f"({bottleneck['avg_duration_seconds']:.3f}s) - {bottleneck['severity'].upper()}")

    # Get recommendations
    print("\n5. Optimization Recommendations:")
    recommendations = profiler.get_optimization_recommendations()
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. {rec}")

    # Export report
    print("\n6. Exporting Performance Report...")
    report_path = "performance_report.json"
    profiler.export_report(report_path)
    print(f"   Report saved to: {report_path}")

    print("\n" + "=" * 80)
    print("ÃƒÂ¢Ã…â€œÃ¢â‚¬Â¦ Performance Profiler test complete!")
    print("=" * 80)
