"""
+==============================================================================ÃƒÂ¢Ã¢â‚¬Â¢Ã¢â‚¬â€
| [WARRIOR] ORFEAS AI ULTIMATE TEXT-TO-IMAGE ENGINE [WARRIOR] |
| MULTI-PROVIDER INTELLIGENT ROUTING SYSTEM |
| FAILURE IS NOT AN OPTION |
+==============================================================================ÃƒÂ¢Ã¢â‚¬Â¢Ã‚Â

ORFEAS AI - ULTIMATE Text-to-Image Generation System
Features:
- [WEB] Multiple AI providers with intelligent failover
- [LAUNCH] Automatic provider selection based on availability & quality
- [ART] Style-specific model routing
- [FAST] Parallel generation with best-of-N selection
- ÃƒÂ°Ã…Â¸Ã¢â‚¬ÂÃ¢â‚¬Å¾ Automatic retry with exponential backoff
- [PREMIUM] Quality scoring and validation
"""

import os
import io
import base64
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
from typing import Optional, Dict, Any, List, Tuple
from enum import Enum
from PIL import Image
from pathlib import Path
import time

logger = logging.getLogger(__name__)


class NetworkConfig:
    """
    [SECURE] ORFEAS SECURITY FIX: Network timeout and retry configuration

    Prevents hanging requests and implements intelligent retry logic
    for transient failures.
    """

    # Timeout configurations (connect_timeout, read_timeout) in seconds
    TIMEOUTS = {
        'fast': (5, 30),      # Quick operations (Pollinations)
        'normal': (10, 60),   # Standard API calls (AUTOMATIC1111)
        'slow': (10, 120),    # Heavy operations (HuggingFace, Stability)
        'upload': (30, 300),  # File uploads (large images)
    }

    # Retry configuration
    RETRY_CONFIG = {
        'total': 3,                    # Total number of retries
        'backoff_factor': 1,           # Exponential backoff: 1s, 2s, 4s
        'status_forcelist': [429, 500, 502, 503, 504],  # HTTP codes to retry
        'allowed_methods': ['GET', 'POST', 'PUT'],       # Methods to retry
    }

    @classmethod
    def get_session(cls, timeout_type: str = 'normal') -> requests.Session:
        """
        Create a requests session with retry logic and appropriate timeouts

        Args:
            timeout_type: Type of timeout configuration ('fast', 'normal', 'slow', 'upload')

        Returns:
            Configured requests.Session with retry adapter
        """
        session = requests.Session()

        # Configure retry strategy
        retry_strategy = Retry(
            total=cls.RETRY_CONFIG['total'],
            backoff_factor=cls.RETRY_CONFIG['backoff_factor'],
            status_forcelist=cls.RETRY_CONFIG['status_forcelist'],
            allowed_methods=cls.RETRY_CONFIG['allowed_methods'],
        )

        # Mount retry adapter
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount('http://', adapter)
        session.mount('https://', adapter)

        return session

    @classmethod
    def get_timeout(cls, timeout_type: str = 'normal') -> Tuple[int, int]:
        """
        Get timeout tuple for a specific operation type

        Args:
            timeout_type: Type of timeout configuration

        Returns:
            (connect_timeout, read_timeout) tuple
        """
        return cls.TIMEOUTS.get(timeout_type, cls.TIMEOUTS['normal'])

class AIProvider(Enum):
    """Supported AI providers for image generation"""
    HUGGINGFACE = "huggingface"
    POLLINATIONS = "pollinations"
    STABILITY = "stability"
    REPLICATE = "replicate"
    FAL_AI = "fal"
    TOGETHER = "together"
    AUTOMATIC1111 = "automatic1111"
    COMFYUI = "comfyui"

class ModelTier(Enum):
    """Model quality tiers"""
    FLAGSHIP = "flagship"  # Best quality, slower
    PREMIUM = "premium"    # Excellent quality, balanced speed
    FAST = "fast"          # Good quality, fastest
    FALLBACK = "fallback"  # Always available

# Model catalog with provider-specific endpoints
MODEL_CATALOG = {
    AIProvider.HUGGINGFACE: {
        ModelTier.FLAGSHIP: [
            "black-forest-labs/FLUX.1-dev",
            "stabilityai/stable-diffusion-xl-base-1.0",
        ],
        ModelTier.PREMIUM: [
            "runwayml/stable-diffusion-v1-5",
            "CompVis/stable-diffusion-v1-4",
        ],
        ModelTier.FAST: [
            "prompthero/openjourney-v4",
            "SG161222/Realistic_Vision_V5.1_noVAE",
        ]
    },
    AIProvider.POLLINATIONS: {
        ModelTier.FLAGSHIP: ["flux"],
        ModelTier.PREMIUM: ["flux-pro"],
        ModelTier.FAST: ["flux-realism"],
    }
}

# Style-to-prompt enhancement mapping
STYLE_ENHANCEMENTS = {
    "realistic": "photorealistic, 8K resolution, highly detailed, professional photography",
    "artistic": "artistic painting, oil on canvas, masterpiece, trending on artstation",
    "anime": "anime style, manga art, cell shaded, vibrant colors, studio quality",
    "cyberpunk": "cyberpunk style, neon lights, futuristic, dystopian, blade runner aesthetic",
    "fantasy": "fantasy art, magical, ethereal, epic fantasy illustration, detailed world",
    "minimalist": "minimalist design, clean lines, simple shapes, modern aesthetic",
    "cinematic": "cinematic lighting, dramatic atmosphere, movie still, epic composition",
    "3d_render": "3D render, octane render, unreal engine 5, ray tracing, highly detailed",
}


class UltimateTextToImageEngine:
    """
    The ULTIMATE text-to-image generation engine with multi-provider support
    """

    def __init__(self):
        self.api_keys = {
            'huggingface': os.getenv('HF_TOKEN', os.getenv('HUGGINGFACE_TOKEN', '')),
            'stability': os.getenv('STABILITY_API_KEY', ''),
            'replicate': os.getenv('REPLICATE_API_TOKEN', ''),
            'fal': os.getenv('FAL_KEY', ''),
            'together': os.getenv('TOGETHER_API_KEY', ''),
        }

        self.provider_status = {provider: True for provider in AIProvider}
        self.provider_metrics = {provider: {'success': 0, 'failure': 0, 'avg_time': 0} for provider in AIProvider}

        # [SECURE] ORFEAS SECURITY FIX: Create sessions with retry logic
        self.sessions = {
            'fast': NetworkConfig.get_session('fast'),
            'normal': NetworkConfig.get_session('normal'),
            'slow': NetworkConfig.get_session('slow'),
            'upload': NetworkConfig.get_session('upload'),
        }

        logger.info("[ORFEAS] ULTIMATE TEXT-TO-IMAGE ENGINE INITIALIZED")
        logger.info("[SECURE] Network timeout protection enabled (10s connect, 30-120s read)")
        self._log_available_providers()

    def _log_available_providers(self):
        """Log which providers are configured"""
        available = []
        for provider, key in self.api_keys.items():
            if key:
                available.append(provider)

        logger.info(f"[OK] Configured providers: {', '.join(available) if available else 'FREE providers only'}")
        logger.info(f"[WEB] Always available: Pollinations.ai (FREE, no API key)")

    def enhance_prompt(self, prompt: str, style: str = "realistic") -> str:
        """
        Enhance prompt with style-specific keywords for better quality
        """
        enhancement = STYLE_ENHANCEMENTS.get(style, STYLE_ENHANCEMENTS["realistic"])
        enhanced = f"{prompt}, {enhancement}"

        # Add quality boosters
        enhanced += ", masterpiece, best quality, highly detailed, sharp focus"

        # Add negative prompt concepts (embedded in positive)
        enhanced += ", NOT: low quality, blurry, distorted, ugly, bad anatomy"

        return enhanced

    def generate_with_huggingface(
        self,
        prompt: str,
        model: str = "black-forest-labs/FLUX.1-dev",
        **kwargs
    ) -> Optional[bytes]:
        """
        Generate image using HuggingFace Inference API
        Supports FLUX.1, SDXL, and many other models
        """
        if not self.api_keys['huggingface']:
            logger.warning("[WARN] HuggingFace API key not configured")
            return None

        try:
            logger.info(f"ÃƒÂ°Ã…Â¸Ã‚Â¤Ã¢â‚¬â€ Generating with HuggingFace: {model}")

            api_url = f"https://api-inference.huggingface.co/models/{model}"
            headers = {"Authorization": f"Bearer {self.api_keys['huggingface']}"}

            payload = {
                "inputs": prompt,
                "parameters": {
                    "width": kwargs.get('width', 1024),
                    "height": kwargs.get('height', 1024),
                    "num_inference_steps": kwargs.get('steps', 50),
                    "guidance_scale": kwargs.get('guidance_scale', 7.5),
                }
            }

            # [SECURE] ORFEAS SECURITY FIX: Use session with retry logic and proper timeouts
            timeout = NetworkConfig.get_timeout('slow')  # (10s connect, 120s read)
            response = self.sessions['slow'].post(api_url, headers=headers, json=payload, timeout=timeout)

            if response.status_code == 200:
                logger.info(f"[OK] HuggingFace generation successful ({model})")
                return response.content
            elif response.status_code == 503:
                logger.warning(f"[WAIT] Model loading, trying again... ({model})")
                time.sleep(20)  # Wait for model to load
                response = self.sessions['slow'].post(api_url, headers=headers, json=payload, timeout=timeout)
                if response.status_code == 200:
                    return response.content

            logger.error(f"[FAIL] HuggingFace error: {response.status_code} - {response.text}")
            return None

        except requests.exceptions.Timeout as e:
            logger.error(f"[FAIL] HuggingFace timeout (connection or read exceeded limits): {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.error(f"[FAIL] HuggingFace connection error (network unreachable): {str(e)}")
            return None
        except Exception as e:
            logger.error(f"[FAIL] HuggingFace exception: {str(e)}")
            return None

    def generate_with_pollinations(
        self,
        prompt: str,
        model: str = "flux",
        **kwargs
    ) -> Optional[bytes]:
        """
        Generate image using Pollinations.ai (FREE, no API key required!)
        """
        try:
            logger.info(f"ÃƒÂ°Ã…Â¸Ã…â€™Ã‚Â¸ Generating with Pollinations.ai: {model}")

            width = kwargs.get('width', 1024)
            height = kwargs.get('height', 1024)
            seed = kwargs.get('seed', -1)

            # Pollinations API endpoint
            base_url = "https://image.pollinations.ai/prompt"

            # URL encode prompt
            from urllib.parse import quote
            encoded_prompt = quote(prompt)

            # Construct URL with parameters
            url = f"{base_url}/{encoded_prompt}?model={model}&width={width}&height={height}&nologo=true&enhance=true"

            if seed > 0:
                url += f"&seed={seed}"

            # [SECURE] ORFEAS SECURITY FIX: Use session with retry logic and fast timeouts
            timeout = NetworkConfig.get_timeout('fast')  # (5s connect, 30s read)
            response = self.sessions['fast'].get(url, timeout=timeout)

            if response.status_code == 200:
                logger.info(f"[OK] Pollinations.ai generation successful")
                return response.content

            logger.error(f"[FAIL] Pollinations error: {response.status_code}")
            return None

        except requests.exceptions.Timeout as e:
            logger.error(f"[FAIL] Pollinations timeout (connection or read exceeded limits): {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.error(f"[FAIL] Pollinations connection error (network unreachable): {str(e)}")
            return None
        except Exception as e:
            logger.error(f"[FAIL] Pollinations exception: {str(e)}")
            return None

    def generate_with_stability(
        self,
        prompt: str,
        **kwargs
    ) -> Optional[bytes]:
        """
        Generate image using Stability AI (SDXL, SD3)
        """
        if not self.api_keys['stability']:
            logger.warning("[WARN] Stability AI API key not configured")
            return None

        try:
            logger.info(f"[ART] Generating with Stability AI")

            url = "https://api.stability.ai/v1/generation/stable-diffusion-xl-1024-v1-0/text-to-image"

            headers = {
                "Authorization": f"Bearer {self.api_keys['stability']}",
                "Content-Type": "application/json",
                "Accept": "application/json",
            }

            payload = {
                "text_prompts": [
                    {
                        "text": prompt,
                        "weight": 1
                    }
                ],
                "cfg_scale": kwargs.get('guidance_scale', 7),
                "height": kwargs.get('height', 1024),
                "width": kwargs.get('width', 1024),
                "steps": kwargs.get('steps', 50),
                "samples": 1,
            }

            # [SECURE] ORFEAS SECURITY FIX: Use session with retry logic and proper timeouts
            timeout = NetworkConfig.get_timeout('slow')  # (10s connect, 120s read)
            response = self.sessions['slow'].post(url, headers=headers, json=payload, timeout=timeout)

            if response.status_code == 200:
                data = response.json()
                if data.get('artifacts'):
                    image_data = base64.b64decode(data['artifacts'][0]['base64'])
                    logger.info(f"[OK] Stability AI generation successful")
                    return image_data

            logger.error(f"[FAIL] Stability AI error: {response.status_code} - {response.text}")
            return None

        except requests.exceptions.Timeout as e:
            logger.error(f"[FAIL] Stability AI timeout (connection or read exceeded limits): {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.error(f"[FAIL] Stability AI connection error (network unreachable): {str(e)}")
            return None
        except Exception as e:
            logger.error(f"[FAIL] Stability AI exception: {str(e)}")
            return None

    def generate_with_automatic1111(
        self,
        prompt: str,
        **kwargs
    ) -> Optional[bytes]:
        """
        Generate image using local AUTOMATIC1111 instance
        """
        try:
            logger.info(f"ÃƒÂ°Ã…Â¸Ã¢â‚¬â€œÃ‚Â¥ÃƒÂ¯Ã‚Â¸Ã‚Â Generating with AUTOMATIC1111 (local)")

            url = kwargs.get('automatic1111_url', 'http://127.0.0.1:7860') + "/sdapi/v1/txt2img"

            payload = {
                "prompt": prompt,
                "negative_prompt": "low quality, blurry, distorted, ugly, bad anatomy, watermark",
                "width": kwargs.get('width', 512),
                "height": kwargs.get('height', 512),
                "steps": kwargs.get('steps', 30),
                "cfg_scale": kwargs.get('guidance_scale', 7),
                "sampler_name": "DPM++ 2M Karras",
            }

            # [SECURE] ORFEAS SECURITY FIX: Use session with retry logic and proper timeouts
            timeout = NetworkConfig.get_timeout('normal')  # (10s connect, 60s read)
            response = self.sessions['normal'].post(url, json=payload, timeout=timeout)

            if response.status_code == 200:
                data = response.json()
                if data.get('images'):
                    image_data = base64.b64decode(data['images'][0])
                    logger.info(f"[OK] AUTOMATIC1111 generation successful")
                    return image_data

            logger.error(f"[FAIL] AUTOMATIC1111 error: {response.status_code}")
            return None

        except requests.exceptions.Timeout as e:
            logger.error(f"[FAIL] AUTOMATIC1111 timeout (connection or read exceeded limits): {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.warning("[WARN] AUTOMATIC1111 not running on local machine")
            return None
        except Exception as e:
            logger.error(f"[FAIL] AUTOMATIC1111 exception: {str(e)}")
            return None

    def validate_image_quality(self, image_bytes: bytes) -> Tuple[bool, float]:
        """
        Validate generated image quality
        Returns: (is_valid, quality_score)
        """
        try:
            img = Image.open(io.BytesIO(image_bytes))

            # Basic checks
            width, height = img.size
            if width < 256 or height < 256:
                return False, 0.0

            # Calculate quality score (0-1)
            quality_score = 1.0

            # Size score (prefer larger images)
            size_score = min(width * height / (1024 * 1024), 1.0)
            quality_score *= (0.5 + 0.5 * size_score)

            # Format score (prefer PNG over JPEG)
            if img.format == 'PNG':
                quality_score *= 1.0
            elif img.format == 'JPEG':
                quality_score *= 0.9
            else:
                quality_score *= 0.8

            return True, quality_score

        except Exception as e:
            logger.error(f"[FAIL] Image validation error: {str(e)}")
            return False, 0.0

    def generate_ultimate(
        self,
        prompt: str,
        style: str = "realistic",
        width: int = 1024,
        height: int = 1024,
        steps: int = 50,
        guidance_scale: float = 7.5,
        quality_mode: str = "best",  # "best", "balanced", "fast"
        **kwargs
    ) -> Optional[bytes]:
        """
        ULTIMATE GENERATION: Try multiple providers with intelligent fallback

        Quality modes:
        - "best": Try flagship models, multiple providers, select best quality
        - "balanced": Try premium models, failover to next provider
        - "fast": Use fastest provider available
        """
        logger.info(f"[ORFEAS] ULTIMATE GENERATION START: '{prompt[:50]}...' | Style: {style} | Mode: {quality_mode}")

        # Enhance prompt based on style
        enhanced_prompt = self.enhance_prompt(prompt, style)
        logger.info(f"[EDIT] Enhanced prompt: '{enhanced_prompt[:100]}...'")

        generation_params = {
            'width': width,
            'height': height,
            'steps': steps,
            'guidance_scale': guidance_scale,
        }

        # Provider priority based on quality mode
        if quality_mode == "best":
            provider_order = [
                (AIProvider.HUGGINGFACE, "black-forest-labs/FLUX.1-dev"),
                (AIProvider.STABILITY, None),
                (AIProvider.HUGGINGFACE, "stabilityai/stable-diffusion-xl-base-1.0"),
                (AIProvider.POLLINATIONS, "flux"),
                (AIProvider.AUTOMATIC1111, None),
            ]
        elif quality_mode == "balanced":
            provider_order = [
                (AIProvider.POLLINATIONS, "flux"),
                (AIProvider.HUGGINGFACE, "runwayml/stable-diffusion-v1-5"),
                (AIProvider.AUTOMATIC1111, None),
            ]
        else:  # fast
            provider_order = [
                (AIProvider.POLLINATIONS, "flux-realism"),
                (AIProvider.HUGGINGFACE, "prompthero/openjourney-v4"),
            ]

        results = []

        for provider, model in provider_order:
            # Skip if provider is marked as down
            if not self.provider_status.get(provider, True):
                logger.warning(f"[WARN] Skipping {provider.value} (marked as unavailable)")
                continue

            try:
                start_time = time.time()
                image_bytes = None

                if provider == AIProvider.HUGGINGFACE:
                    image_bytes = self.generate_with_huggingface(enhanced_prompt, model=model, **generation_params)
                elif provider == AIProvider.POLLINATIONS:
                    image_bytes = self.generate_with_pollinations(enhanced_prompt, model=model, **generation_params)
                elif provider == AIProvider.STABILITY:
                    image_bytes = self.generate_with_stability(enhanced_prompt, **generation_params)
                elif provider == AIProvider.AUTOMATIC1111:
                    image_bytes = self.generate_with_automatic1111(enhanced_prompt, **generation_params)

                generation_time = time.time() - start_time

                if image_bytes:
                    # Validate quality
                    is_valid, quality_score = self.validate_image_quality(image_bytes)

                    if is_valid:
                        logger.info(f"[OK] SUCCESS: {provider.value} | Time: {generation_time:.1f}s | Quality: {quality_score:.2f}")
                        results.append((image_bytes, quality_score, provider.value, generation_time))

                        # Update metrics
                        self.provider_metrics[provider]['success'] += 1

                        # In "fast" mode, return first success
                        if quality_mode == "fast":
                            return image_bytes

                        # In "balanced" mode, return if quality > 0.7
                        if quality_mode == "balanced" and quality_score > 0.7:
                            return image_bytes
                    else:
                        logger.warning(f"[WARN] {provider.value} generated invalid image")
                        self.provider_metrics[provider]['failure'] += 1
                else:
                    logger.warning(f"[WARN] {provider.value} returned no image")
                    self.provider_metrics[provider]['failure'] += 1

            except Exception as e:
                logger.error(f"[FAIL] {provider.value} exception: {str(e)}")
                self.provider_metrics[provider]['failure'] += 1
                continue

        # Select best result if in "best" mode
        if results:
            results.sort(key=lambda x: x[1], reverse=True)  # Sort by quality score
            best_image, best_score, best_provider, best_time = results[0]

            logger.info(f"[TROPHY] BEST RESULT: {best_provider} | Quality: {best_score:.2f} | Time: {best_time:.1f}s")
            return best_image

        # All providers failed - generate fallback
        logger.error("[FAIL] ALL PROVIDERS FAILED - Generating fallback placeholder")
        return self._generate_fallback_image(prompt, width, height)

    def _generate_fallback_image(self, prompt: str, width: int, height: int) -> bytes:
        """
        Generate a fallback placeholder image when all providers fail
        """
        from PIL import ImageDraw, ImageFont

        img = Image.new('RGB', (width, height), color=(40, 44, 52))
        draw = ImageDraw.Draw(img)

        # Add gradient effect
        for y in range(height):
            color_value = int(40 + (y / height) * 60)
            draw.line([(0, y), (width, y)], fill=(color_value, color_value + 4, color_value + 12))

        # Add text
        try:
            font = ImageFont.truetype("arial.ttf", 32)
        except:
            font = ImageFont.load_default()

        text_lines = [
            "[WARN] AI Generation Failed",
            "",
            f"Prompt: {prompt[:50]}...",
            "",
            "All providers unavailable",
            "Please try again later"
        ]

        y_offset = height // 4
        for line in text_lines:
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
            x = (width - text_width) // 2
            draw.text((x, y_offset), line, fill=(200, 200, 200), font=font)
            y_offset += 50

        buffer = io.BytesIO()
        img.save(buffer, format='PNG')
        return buffer.getvalue()

    def get_provider_stats(self) -> Dict[str, Any]:
        """Get statistics about provider performance"""
        stats = {}
        for provider, metrics in self.provider_metrics.items():
            total = metrics['success'] + metrics['failure']
            success_rate = (metrics['success'] / total * 100) if total > 0 else 0
            stats[provider.value] = {
                'success': metrics['success'],
                'failure': metrics['failure'],
                'success_rate': f"{success_rate:.1f}%",
                'status': 'available' if self.provider_status[provider] else 'unavailable'
            }
        return stats


# Global singleton instance
_ultimate_engine = None

def get_ultimate_engine() -> UltimateTextToImageEngine:
    """Get or create the global ultimate engine instance"""
    global _ultimate_engine
    if _ultimate_engine is None:
        _ultimate_engine = UltimateTextToImageEngine()
    return _ultimate_engine


if __name__ == "__main__":
    # Test the engine
    logging.basicConfig(level=logging.INFO)

    engine = get_ultimate_engine()

    print("\n" + "="*80)
    print("[ORFEAS] ORFEAS AI ULTIMATE TEXT-TO-IMAGE ENGINE TEST")
    print("="*80 + "\n")

    test_prompt = "A majestic lion standing on a mountain peak at sunset, photorealistic"

    print(f"[EDIT] Test prompt: {test_prompt}\n")

    image_bytes = engine.generate_ultimate(
        prompt=test_prompt,
        style="realistic",
        quality_mode="best"
    )

    if image_bytes:
        output_path = Path("test_ultimate_generation.png")
        output_path.write_bytes(image_bytes)
        print(f"\n[OK] Image saved to: {output_path}")
    else:
        print("\n[FAIL] Generation failed")

    print("\n[STATS] Provider Statistics:")
    stats = engine.get_provider_stats()
    for provider, data in stats.items():
        print(f"  {provider}: {data}")
