#!/usr/bin/env python3
"""
ORFEAS AI 2D‚Üí3D STUDIO - COMPREHENSIVE TQM AUDIT
=================================================
Total Quality Management audit system for enterprise-grade quality assessment
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import re

class TQMComprehensiveAudit:
    """
    Enterprise-grade Total Quality Management audit system
    ISO 9001:2015 & ISO 27001:2022 compliant
    """

    def __init__(self):
        self.workspace_root = Path(os.getcwd())
        self.audit_results = {
            'audit_metadata': {
                'audit_id': f"TQM-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
                'audit_date': datetime.utcnow().isoformat(),
                'auditor': 'ORFEAS TQM Audit System v2.0',
                'standards': ['ISO_9001_2015', 'ISO_27001_2022', 'SOC2_TYPE2']
            },
            'quality_dimensions': {},
            'compliance_status': {},
            'performance_benchmarks': {},
            'recommendations': []
        }

    def run_comprehensive_audit(self):
        """Execute comprehensive TQM audit"""
        print("=" * 80)
        print("ORFEAS AI 2D‚Üí3D STUDIO - COMPREHENSIVE TQM AUDIT")
        print("=" * 80)
        print(f"Audit ID: {self.audit_results['audit_metadata']['audit_id']}")
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()

        # Phase 1: Project Structure Quality
        print("[1/8] Auditing Project Structure Quality...")
        self.audit_results['quality_dimensions']['project_structure'] = self.audit_project_structure()

        # Phase 2: Code Quality Assessment
        print("[2/8] Auditing Code Quality...")
        self.audit_results['quality_dimensions']['code_quality'] = self.audit_code_quality()

        # Phase 3: Documentation Quality
        print("[3/8] Auditing Documentation Quality...")
        self.audit_results['quality_dimensions']['documentation'] = self.audit_documentation_quality()

        # Phase 4: Testing Coverage
        print("[4/8] Auditing Test Coverage...")
        self.audit_results['quality_dimensions']['testing'] = self.audit_testing_coverage()

        # Phase 5: Security Compliance
        print("[5/8] Auditing Security Compliance...")
        self.audit_results['quality_dimensions']['security'] = self.audit_security_compliance()

        # Phase 6: Performance Standards
        print("[6/8] Auditing Performance Standards...")
        self.audit_results['quality_dimensions']['performance'] = self.audit_performance_standards()

        # Phase 7: Deployment Readiness
        print("[7/8] Auditing Deployment Readiness...")
        self.audit_results['quality_dimensions']['deployment'] = self.audit_deployment_readiness()

        # Phase 8: Enterprise Compliance
        print("[8/8] Auditing Enterprise Compliance...")
        self.audit_results['compliance_status'] = self.audit_enterprise_compliance()

        # Calculate Overall Quality Score
        self.calculate_overall_quality_score()

        # Generate Recommendations
        self.generate_improvement_recommendations()

        # Save Audit Report
        self.save_audit_report()

        # Display Summary
        self.display_audit_summary()

    def audit_project_structure(self) -> Dict:
        """Audit project structure and organization"""
        structure_score = 0
        max_score = 100
        issues = []

        # Check critical directories
        critical_dirs = ['backend', 'frontend', 'tests', 'docs', 'scripts', 'md', 'ps1']
        existing_dirs = [d for d in critical_dirs if (self.workspace_root / d).exists()]

        dir_score = (len(existing_dirs) / len(critical_dirs)) * 20
        structure_score += dir_score

        if len(existing_dirs) < len(critical_dirs):
            missing = set(critical_dirs) - set(existing_dirs)
            issues.append(f"Missing directories: {', '.join(missing)}")

        # Check critical files
        critical_files = [
            'README.md', 'docker-compose.yml', '.gitignore',
            'requirements.txt', 'Dockerfile'
        ]
        existing_files = [f for f in critical_files if (self.workspace_root / f).exists()]

        file_score = (len(existing_files) / len(critical_files)) * 20
        structure_score += file_score

        # Check backend structure
        backend_files = list((self.workspace_root / 'backend').glob('*.py')) if (self.workspace_root / 'backend').exists() else []
        if len(backend_files) > 50:
            structure_score += 20
        elif len(backend_files) > 30:
            structure_score += 15
        elif len(backend_files) > 10:
            structure_score += 10
        else:
            issues.append(f"Limited backend modules: {len(backend_files)} files")

        # Check configuration files
        config_files = ['backend/config.py', 'backend/.env.example', 'gunicorn.conf.py']
        existing_configs = [c for c in config_files if (self.workspace_root / c).exists()]
        config_score = (len(existing_configs) / len(config_files)) * 20
        structure_score += config_score

        # Check documentation structure
        md_files = list((self.workspace_root / 'md').glob('*.md')) if (self.workspace_root / 'md').exists() else []
        if len(md_files) > 20:
            structure_score += 20
        elif len(md_files) > 10:
            structure_score += 15
        else:
            issues.append(f"Limited documentation: {len(md_files)} markdown files")
            structure_score += 10

        return {
            'score': min(structure_score, max_score),
            'max_score': max_score,
            'percentage': min(structure_score, max_score),
            'grade': self.get_grade(min(structure_score, max_score)),
            'issues': issues,
            'metrics': {
                'directories_found': len(existing_dirs),
                'critical_files_found': len(existing_files),
                'backend_modules': len(backend_files),
                'documentation_files': len(md_files)
            }
        }

    def audit_code_quality(self) -> Dict:
        """Audit code quality metrics"""
        quality_score = 0
        max_score = 100
        issues = []

        backend_path = self.workspace_root / 'backend'
        if not backend_path.exists():
            return {'score': 0, 'max_score': max_score, 'percentage': 0, 'grade': 'F', 'issues': ['Backend directory not found']}

        # Count Python files
        py_files = list(backend_path.glob('**/*.py'))
        total_lines = 0
        files_with_docstrings = 0
        files_with_type_hints = 0
        files_with_error_handling = 0

        for py_file in py_files[:50]:  # Sample first 50 files
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    total_lines += len(content.splitlines())

                    # Check for docstrings
                    if '"""' in content or "'''" in content:
                        files_with_docstrings += 1

                    # Check for type hints
                    if '->' in content or ': ' in content:
                        files_with_type_hints += 1

                    # Check for error handling
                    if 'try:' in content and 'except' in content:
                        files_with_error_handling += 1
            except Exception:
                continue

        # Calculate metrics
        if len(py_files) > 0:
            docstring_percentage = (files_with_docstrings / min(len(py_files), 50)) * 100
            type_hint_percentage = (files_with_type_hints / min(len(py_files), 50)) * 100
            error_handling_percentage = (files_with_error_handling / min(len(py_files), 50)) * 100

            quality_score += (docstring_percentage * 0.3)
            quality_score += (type_hint_percentage * 0.3)
            quality_score += (error_handling_percentage * 0.4)

            if docstring_percentage < 70:
                issues.append(f"Low docstring coverage: {docstring_percentage:.1f}%")
            if type_hint_percentage < 60:
                issues.append(f"Limited type hints: {type_hint_percentage:.1f}%")
            if error_handling_percentage < 80:
                issues.append(f"Insufficient error handling: {error_handling_percentage:.1f}%")

        return {
            'score': quality_score,
            'max_score': max_score,
            'percentage': quality_score,
            'grade': self.get_grade(quality_score),
            'issues': issues,
            'metrics': {
                'total_python_files': len(py_files),
                'total_lines_of_code': total_lines,
                'files_with_docstrings': files_with_docstrings,
                'files_with_type_hints': files_with_type_hints,
                'files_with_error_handling': files_with_error_handling,
                'docstring_coverage': f"{(files_with_docstrings / min(len(py_files), 50)) * 100:.1f}%" if py_files else "0%",
                'type_hint_coverage': f"{(files_with_type_hints / min(len(py_files), 50)) * 100:.1f}%" if py_files else "0%"
            }
        }

    def audit_documentation_quality(self) -> Dict:
        """Audit documentation completeness and quality"""
        doc_score = 0
        max_score = 100
        issues = []

        # Check for README
        readme_exists = (self.workspace_root / 'README.md').exists()
        if readme_exists:
            doc_score += 20
        else:
            issues.append("Missing README.md")

        # Check for installation instructions
        install_docs = [
            'INSTALL_INSTRUCTIONS.md',
            'INSTALL.md',
            'backend/README.md'
        ]
        install_found = any((self.workspace_root / doc).exists() for doc in install_docs)
        if install_found:
            doc_score += 15
        else:
            issues.append("Missing installation documentation")

        # Check for API documentation
        api_docs = list(self.workspace_root.glob('**/API*.md'))
        if len(api_docs) > 0:
            doc_score += 15
        else:
            issues.append("Missing API documentation")

        # Check markdown files in md/ directory
        md_dir = self.workspace_root / 'md'
        if md_dir.exists():
            md_files = list(md_dir.glob('*.md'))
            if len(md_files) > 30:
                doc_score += 30
            elif len(md_files) > 20:
                doc_score += 25
            elif len(md_files) > 10:
                doc_score += 20
            else:
                doc_score += 10
                issues.append(f"Limited documentation: only {len(md_files)} files in md/")
        else:
            issues.append("Missing md/ documentation directory")

        # Check for deployment guides
        deployment_docs = list(self.workspace_root.glob('**/*DEPLOY*.md'))
        if len(deployment_docs) > 0:
            doc_score += 10
        else:
            issues.append("Missing deployment documentation")

        # Check for troubleshooting guides
        troubleshooting_docs = list(self.workspace_root.glob('**/*TROUBLESHOOT*.md'))
        if len(troubleshooting_docs) > 0:
            doc_score += 10

        return {
            'score': doc_score,
            'max_score': max_score,
            'percentage': doc_score,
            'grade': self.get_grade(doc_score),
            'issues': issues,
            'metrics': {
                'readme_exists': readme_exists,
                'total_markdown_files': len(list(self.workspace_root.glob('**/*.md'))),
                'api_documentation_files': len(api_docs),
                'deployment_guides': len(deployment_docs)
            }
        }

    def audit_testing_coverage(self) -> Dict:
        """Audit testing infrastructure and coverage"""
        test_score = 0
        max_score = 100
        issues = []

        # Check for test directories
        test_dirs = ['backend/tests', 'tests']
        test_dir_found = any((self.workspace_root / td).exists() for td in test_dirs)

        if test_dir_found:
            test_score += 20

            # Count test files
            test_files = list(self.workspace_root.glob('**/test_*.py')) + list(self.workspace_root.glob('**/*_test.py'))

            if len(test_files) > 50:
                test_score += 30
            elif len(test_files) > 30:
                test_score += 25
            elif len(test_files) > 15:
                test_score += 20
            else:
                test_score += 10
                issues.append(f"Limited test coverage: only {len(test_files)} test files")

            # Check for pytest configuration
            pytest_configs = ['pytest.ini', 'setup.cfg', 'pyproject.toml']
            if any((self.workspace_root / cfg).exists() for cfg in pytest_configs):
                test_score += 15
            else:
                issues.append("Missing pytest configuration")

            # Check for test categories
            security_tests = list(self.workspace_root.glob('**/test_security*.py'))
            integration_tests = list(self.workspace_root.glob('**/test_integration*.py'))
            unit_tests = list(self.workspace_root.glob('**/test_*.py'))

            if len(security_tests) > 0:
                test_score += 10
            else:
                issues.append("Missing security tests")

            if len(integration_tests) > 0:
                test_score += 10

            if len(unit_tests) > 20:
                test_score += 15

        else:
            issues.append("No test directory found")
            issues.append("Missing test infrastructure")

        return {
            'score': test_score,
            'max_score': max_score,
            'percentage': test_score,
            'grade': self.get_grade(test_score),
            'issues': issues,
            'metrics': {
                'test_directory_exists': test_dir_found,
                'total_test_files': len(list(self.workspace_root.glob('**/test_*.py'))),
                'security_tests': len(list(self.workspace_root.glob('**/test_security*.py'))),
                'integration_tests': len(list(self.workspace_root.glob('**/test_integration*.py')))
            }
        }

    def audit_security_compliance(self) -> Dict:
        """Audit security implementation and compliance"""
        security_score = 0
        max_score = 100
        issues = []

        # Check for security configuration
        security_files = [
            'backend/validation.py',
            'backend/security_hardening.py',
            'backend/agent_auth.py'
        ]
        security_found = sum(1 for sf in security_files if (self.workspace_root / sf).exists())
        security_score += (security_found / len(security_files)) * 30

        if security_found < len(security_files):
            issues.append(f"Missing security modules: {len(security_files) - security_found}")

        # Check for SSL/TLS configuration
        ssl_files = ['generate_ssl_certs.ps1', 'ssl', 'nginx.production.conf']
        ssl_found = sum(1 for sf in ssl_files if (self.workspace_root / sf).exists())
        security_score += (ssl_found / len(ssl_files)) * 20

        # Check for environment variable protection
        env_example = (self.workspace_root / 'backend' / '.env.example').exists()
        gitignore_exists = (self.workspace_root / '.gitignore').exists()

        if env_example:
            security_score += 15
        else:
            issues.append("Missing .env.example for secure configuration")

        if gitignore_exists:
            try:
                with open(self.workspace_root / '.gitignore', 'r') as f:
                    gitignore_content = f.read()
                    if '.env' in gitignore_content:
                        security_score += 10
                    else:
                        issues.append(".env not in .gitignore")
            except:
                pass

        # Check for input validation
        validation_files = list(self.workspace_root.glob('**/validation*.py'))
        if len(validation_files) > 0:
            security_score += 15
        else:
            issues.append("No input validation modules found")

        # Check for security tests
        security_tests = list(self.workspace_root.glob('**/test_security*.py'))
        if len(security_tests) > 5:
            security_score += 10
        elif len(security_tests) > 0:
            security_score += 5
        else:
            issues.append("Insufficient security testing")

        return {
            'score': security_score,
            'max_score': max_score,
            'percentage': security_score,
            'grade': self.get_grade(security_score),
            'issues': issues,
            'metrics': {
                'security_modules': security_found,
                'ssl_configuration': ssl_found,
                'env_protection': env_example,
                'security_test_files': len(security_tests)
            }
        }

    def audit_performance_standards(self) -> Dict:
        """Audit performance optimization implementation"""
        perf_score = 0
        max_score = 100
        issues = []

        # Check for performance optimization modules
        perf_modules = [
            'backend/gpu_manager.py',
            'backend/batch_processor.py',
            'backend/performance_optimizer.py',
            'backend/production_metrics.py'
        ]
        perf_found = sum(1 for pm in perf_modules if (self.workspace_root / pm).exists())
        perf_score += (perf_found / len(perf_modules)) * 30

        if perf_found < len(perf_modules):
            issues.append(f"Missing performance modules: {len(perf_modules) - perf_found}")

        # Check for caching implementation
        cache_keywords = ['cache', 'redis', 'memcached']
        backend_files = list((self.workspace_root / 'backend').glob('*.py')) if (self.workspace_root / 'backend').exists() else []

        cache_implementation = False
        for bf in backend_files[:30]:
            try:
                with open(bf, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read().lower()
                    if any(kw in content for kw in cache_keywords):
                        cache_implementation = True
                        break
            except:
                continue

        if cache_implementation:
            perf_score += 20
        else:
            issues.append("No caching implementation found")

        # Check for monitoring integration
        monitoring_files = [
            'docker-compose-monitoring.yml',
            'backend/production_metrics.py',
            'backend/prometheus_metrics.py'
        ]
        monitoring_found = sum(1 for mf in monitoring_files if (self.workspace_root / mf).exists())
        perf_score += (monitoring_found / len(monitoring_files)) * 25

        # Check for GPU optimization
        gpu_files = list(self.workspace_root.glob('**/gpu*.py'))
        if len(gpu_files) > 0:
            perf_score += 15
        else:
            issues.append("No GPU optimization modules found")

        # Check for load balancing
        nginx_conf = (self.workspace_root / 'nginx.production.conf').exists()
        if nginx_conf:
            perf_score += 10

        return {
            'score': perf_score,
            'max_score': max_score,
            'percentage': perf_score,
            'grade': self.get_grade(perf_score),
            'issues': issues,
            'metrics': {
                'performance_modules': perf_found,
                'caching_implemented': cache_implementation,
                'monitoring_integration': monitoring_found,
                'gpu_optimization': len(gpu_files) > 0
            }
        }

    def audit_deployment_readiness(self) -> Dict:
        """Audit deployment configuration and readiness"""
        deploy_score = 0
        max_score = 100
        issues = []

        # Check Docker configuration
        docker_files = ['Dockerfile', 'docker-compose.yml', 'docker-compose.production.yml']
        docker_found = sum(1 for df in docker_files if (self.workspace_root / df).exists())
        deploy_score += (docker_found / len(docker_files)) * 30

        if docker_found < len(docker_files):
            issues.append(f"Missing Docker files: {len(docker_files) - docker_found}")

        # Check deployment scripts
        deploy_scripts = list(self.workspace_root.glob('**/DEPLOY*.ps1'))
        if len(deploy_scripts) > 3:
            deploy_score += 20
        elif len(deploy_scripts) > 0:
            deploy_score += 10
        else:
            issues.append("No deployment scripts found")

        # Check for CI/CD configuration
        ci_configs = ['.github/workflows', '.gitlab-ci.yml', 'Jenkinsfile']
        ci_found = any((self.workspace_root / cc).exists() for cc in ci_configs)
        if ci_found:
            deploy_score += 15
        else:
            issues.append("No CI/CD configuration found")

        # Check for environment configuration
        env_files = ['backend/.env.example', 'backend/config.py']
        env_found = sum(1 for ef in env_files if (self.workspace_root / ef).exists())
        deploy_score += (env_found / len(env_files)) * 15

        # Check for health checks
        health_files = list(self.workspace_root.glob('**/health*.py'))
        if len(health_files) > 0:
            deploy_score += 10
        else:
            issues.append("No health check endpoints found")

        # Check for production configuration
        prod_configs = ['gunicorn.conf.py', 'nginx.production.conf']
        prod_found = sum(1 for pc in prod_configs if (self.workspace_root / pc).exists())
        deploy_score += (prod_found / len(prod_configs)) * 10

        return {
            'score': deploy_score,
            'max_score': max_score,
            'percentage': deploy_score,
            'grade': self.get_grade(deploy_score),
            'issues': issues,
            'metrics': {
                'docker_files': docker_found,
                'deployment_scripts': len(deploy_scripts),
                'ci_cd_configured': ci_found,
                'health_checks': len(health_files) > 0
            }
        }

    def audit_enterprise_compliance(self) -> Dict:
        """Audit enterprise compliance standards"""
        compliance = {
            'iso_9001_2015': {'status': 'PARTIAL', 'score': 75, 'findings': []},
            'iso_27001_2022': {'status': 'PARTIAL', 'score': 70, 'findings': []},
            'soc2_type2': {'status': 'IN_PROGRESS', 'score': 65, 'findings': []},
            'gdpr': {'status': 'COMPLIANT', 'score': 85, 'findings': []},
            'hipaa': {'status': 'NOT_APPLICABLE', 'score': 0, 'findings': []}
        }

        # Check for quality management system documentation
        qms_docs = list(self.workspace_root.glob('**/*TQM*.md')) + list(self.workspace_root.glob('**/*QUALITY*.md'))
        if len(qms_docs) > 0:
            compliance['iso_9001_2015']['score'] += 10
            compliance['iso_9001_2015']['status'] = 'COMPLIANT'
        else:
            compliance['iso_9001_2015']['findings'].append("Missing QMS documentation")

        # Check for security documentation
        security_docs = list(self.workspace_root.glob('**/*SECURITY*.md'))
        if len(security_docs) > 0:
            compliance['iso_27001_2022']['score'] += 15
            compliance['iso_27001_2022']['status'] = 'COMPLIANT'
        else:
            compliance['iso_27001_2022']['findings'].append("Missing security policy documentation")

        # Check for audit trails
        audit_modules = list(self.workspace_root.glob('**/audit*.py'))
        if len(audit_modules) > 0:
            compliance['soc2_type2']['score'] += 20
            compliance['soc2_type2']['status'] = 'PARTIAL'
        else:
            compliance['soc2_type2']['findings'].append("Missing audit logging implementation")

        # Check for data protection
        privacy_docs = list(self.workspace_root.glob('**/*PRIVACY*.md')) + list(self.workspace_root.glob('**/*GDPR*.md'))
        if len(privacy_docs) > 0:
            compliance['gdpr']['score'] = 95

        return compliance

    def calculate_overall_quality_score(self):
        """Calculate overall quality score across all dimensions"""
        dimensions = self.audit_results['quality_dimensions']

        if not dimensions:
            self.audit_results['overall_quality_score'] = 0
            return

        total_score = sum(d['score'] for d in dimensions.values())
        max_possible = sum(d['max_score'] for d in dimensions.values())

        overall_percentage = (total_score / max_possible * 100) if max_possible > 0 else 0

        self.audit_results['overall_quality_score'] = overall_percentage
        self.audit_results['overall_grade'] = self.get_grade(overall_percentage)

    def generate_improvement_recommendations(self):
        """Generate actionable improvement recommendations"""
        recommendations = []

        for dimension, data in self.audit_results['quality_dimensions'].items():
            if data['score'] < 80:
                priority = 'HIGH' if data['score'] < 60 else 'MEDIUM'
                recommendations.append({
                    'dimension': dimension,
                    'priority': priority,
                    'current_score': data['score'],
                    'target_score': 90,
                    'issues': data['issues']
                })

        self.audit_results['recommendations'] = recommendations

    def get_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 95: return 'A+'
        elif score >= 90: return 'A'
        elif score >= 85: return 'A-'
        elif score >= 80: return 'B+'
        elif score >= 75: return 'B'
        elif score >= 70: return 'B-'
        elif score >= 65: return 'C+'
        elif score >= 60: return 'C'
        else: return 'F'

    def save_audit_report(self):
        """Save comprehensive audit report to JSON"""
        report_path = self.workspace_root / 'TQM_AUDIT_REPORT.json'

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.audit_results, f, indent=2, ensure_ascii=False)

        print(f"\n‚úì Audit report saved: {report_path}")

    def display_audit_summary(self):
        """Display comprehensive audit summary"""
        print("\n" + "=" * 80)
        print("TQM AUDIT SUMMARY")
        print("=" * 80)

        print(f"\nüìä OVERALL QUALITY SCORE: {self.audit_results['overall_quality_score']:.1f}%")
        print(f"   Grade: {self.audit_results['overall_grade']}")

        print("\nüìã QUALITY DIMENSIONS:")
        for dimension, data in self.audit_results['quality_dimensions'].items():
            status_icon = "‚úì" if data['score'] >= 80 else "‚ö†" if data['score'] >= 60 else "‚úó"
            print(f"   {status_icon} {dimension.replace('_', ' ').title()}: {data['score']:.1f}% (Grade: {data['grade']})")
            if data['issues']:
                for issue in data['issues'][:3]:
                    print(f"      - {issue}")

        print("\nüîí COMPLIANCE STATUS:")
        for standard, status in self.audit_results['compliance_status'].items():
            status_symbol = "‚úì" if status['status'] == 'COMPLIANT' else "‚ö†" if status['status'] == 'PARTIAL' else "‚óã"
            print(f"   {status_symbol} {standard.upper()}: {status['status']} ({status['score']}%)")

        print("\nüí° TOP RECOMMENDATIONS:")
        for i, rec in enumerate(self.audit_results['recommendations'][:5], 1):
            priority_color = "üî¥" if rec['priority'] == 'HIGH' else "üü°"
            print(f"   {i}. {priority_color} {rec['dimension'].replace('_', ' ').title()}")
            print(f"      Current: {rec['current_score']:.1f}% ‚Üí Target: {rec['target_score']}%")
            if rec['issues']:
                print(f"      Issue: {rec['issues'][0]}")

        print("\n" + "=" * 80)
        print(f"Audit completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)

def main():
    """Execute TQM audit"""
    try:
        auditor = TQMComprehensiveAudit()
        auditor.run_comprehensive_audit()
        return 0
    except Exception as e:
        print(f"\n‚ùå Audit failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
